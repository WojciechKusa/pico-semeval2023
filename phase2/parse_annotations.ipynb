{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b0ca03",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79ddb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import collections\n",
    "import re\n",
    "import spacy\n",
    "import scispacy\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4a8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers\n",
    "\n",
    "#loading the scispacy model\n",
    "nlp = spacy.load('en_core_sci_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed802f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file\n",
    "\n",
    "file_path_train = '/mnt/nas2/data/systematicReview/semeval2023/data/st2_train_inc_text.csv'\n",
    "file_path_test = '/mnt/nas2/data/systematicReview/semeval2023/data/st2_test_inc_text.csv'\n",
    "\n",
    "df_train = pd.read_csv(file_path_train, sep=',')\n",
    "train_df = df_train.to_dict('records')\n",
    "\n",
    "df_test = pd.read_csv(file_path_test, sep=',')\n",
    "test_df = df_test.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836cd66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adcbedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>stage2_labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sn9u41</td>\n",
       "      <td>t5_2s23e</td>\n",
       "      <td>I read an old thread on here that someone said...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Tysabri experiences\\nHi all\\n\\nI just had my 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p7j49y</td>\n",
       "      <td>t5_2syer</td>\n",
       "      <td>I have read that gout can't be cured, that it'...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Crazy amount of sardines caused gout (possibly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smgy0q</td>\n",
       "      <td>t5_2s3g1</td>\n",
       "      <td>I always read stories of people who suffer fro...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Im sorry for intruding but I just want to say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sxglhl</td>\n",
       "      <td>t5_2s3g1</td>\n",
       "      <td>Our results indicate that the addition of prob...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Is Bacillus coagulans supplementation plus low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rxyk1d</td>\n",
       "      <td>t5_2s1h9</td>\n",
       "      <td>Ive read that amnesia always followed a tonic ...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>[deleted by user]\\n[removed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id subreddit_id                                              claim  \\\n",
       "0  sn9u41     t5_2s23e  I read an old thread on here that someone said...   \n",
       "1  p7j49y     t5_2syer  I have read that gout can't be cured, that it'...   \n",
       "2  smgy0q     t5_2s3g1  I always read stories of people who suffer fro...   \n",
       "3  sxglhl     t5_2s3g1  Our results indicate that the addition of prob...   \n",
       "4  rxyk1d     t5_2s1h9  Ive read that amnesia always followed a tonic ...   \n",
       "\n",
       "                                       stage2_labels  \\\n",
       "0  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "1  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "2  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "3  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "4  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "\n",
       "                                                text  \n",
       "0  Tysabri experiences\\nHi all\\n\\nI just had my 3...  \n",
       "1  Crazy amount of sardines caused gout (possibly...  \n",
       "2  Im sorry for intruding but I just want to say ...  \n",
       "3  Is Bacillus coagulans supplementation plus low...  \n",
       "4                       [deleted by user]\\n[removed]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41bcc45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s8qa1c</td>\n",
       "      <td>t5_2tyg2</td>\n",
       "      <td>[Structural dissociation](https://did-research...</td>\n",
       "      <td>Can structural dissociation lead to psychosis?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opt7bh</td>\n",
       "      <td>t5_2syer</td>\n",
       "      <td>I though that the gout was for old whiskey dri...</td>\n",
       "      <td>First time gout flare\\nHi guys,\\n\\nJust turned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rfj6tf</td>\n",
       "      <td>t5_2qlaa</td>\n",
       "      <td>I've read that a liquid alginate suspension (G...</td>\n",
       "      <td>Can I buy liquid alginate suspension (Gaviscon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qyfer0</td>\n",
       "      <td>t5_2syer</td>\n",
       "      <td>This will reduce the effects of Lisinopril pre...</td>\n",
       "      <td>[deleted by user]\\n[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rr99la</td>\n",
       "      <td>t5_2qlaa</td>\n",
       "      <td>all the gerd/lpr stuff online says we should a...</td>\n",
       "      <td>I seem to reflux more and sleep worse when I d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id subreddit_id                                              claim  \\\n",
       "0  s8qa1c     t5_2tyg2  [Structural dissociation](https://did-research...   \n",
       "1  opt7bh     t5_2syer  I though that the gout was for old whiskey dri...   \n",
       "2  rfj6tf     t5_2qlaa  I've read that a liquid alginate suspension (G...   \n",
       "3  qyfer0     t5_2syer  This will reduce the effects of Lisinopril pre...   \n",
       "4  rr99la     t5_2qlaa  all the gerd/lpr stuff online says we should a...   \n",
       "\n",
       "                                                text  \n",
       "0  Can structural dissociation lead to psychosis?...  \n",
       "1  First time gout flare\\nHi guys,\\n\\nJust turned...  \n",
       "2  Can I buy liquid alginate suspension (Gaviscon...  \n",
       "3                       [deleted by user]\\n[removed]  \n",
       "4  I seem to reflux more and sleep worse when I d...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba23a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_list = ['[deleted]', '[removed]', 'deleted by user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b1dcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "picos_mapping = {'population': 1, 'intervention':2, 'outcome':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322dcd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_labels(df):\n",
    "    \n",
    "    # parse dataframe and fetch annotations in a dict\n",
    "\n",
    "    labels = []\n",
    "    claim_offsets = []\n",
    "\n",
    "    for counter, row in enumerate(df):\n",
    "        #print('--------------------------------------------------------')\n",
    "\n",
    "        reddit_id = row['subreddit_id']\n",
    "\n",
    "        post_id = row['post_id']\n",
    "        #print( post_id )\n",
    "\n",
    "        claim = row['claim']\n",
    "        #print('Claim: ', claim)\n",
    "\n",
    "        full_text = row['text']\n",
    "        #print('Full-Text: ', full_text)         \n",
    "\n",
    "        if any(word in full_text for word in deleted_list):\n",
    "            # If the post was removed by the user\n",
    "            labels.append('N.A.')\n",
    "            claim_offsets.append('N.A.')\n",
    "        else:\n",
    "            # If the post was not removed by the user\n",
    "            # Get entities\n",
    "            stage2_labels = ast.literal_eval( row['stage2_labels']  )\n",
    "            #print( 'MAIN:     ', stage2_labels )\n",
    "            stage2_labels = stage2_labels[0]['crowd-entity-annotation']['entities']\n",
    "            #print( 'OFFSHOOT:     ', stage2_labels )\n",
    "\n",
    "            # Get Char indices\n",
    "            full_text_indices = [ counter for counter, i in enumerate(full_text) ]\n",
    "            #print( 'full_text_indices:     ', full_text_indices )\n",
    "\n",
    "            label_each_char = [0] * len(full_text) # Generate a 0 label for each character in the full text\n",
    "            \n",
    "            claim_start = full_text.index(claim)\n",
    "            claim_end = claim_start + len(claim)\n",
    "\n",
    "            for l in stage2_labels:\n",
    "                extrct_annot = row['text'][ l['startOffset'] : l['endOffset'] ]\n",
    "                pico =  l['label']\n",
    "\n",
    "                # Are the start and stop offsets in the full-text offsets?\n",
    "                if l['startOffset'] in full_text_indices:\n",
    "\n",
    "                    prev_length = len(label_each_char)\n",
    "                    start = l['startOffset']\n",
    "                    end = l['startOffset']+(len(extrct_annot))\n",
    "                    label_indices = [ i for i in range(start, end) ]\n",
    "\n",
    "                    for i in range( start, end ):\n",
    "                        old_label = label_each_char[i]\n",
    "                        new_label = picos_mapping[pico]\n",
    "                        if new_label > old_label:\n",
    "                            label_each_char[i] = new_label\n",
    "                    assert len(label_each_char) == prev_length\n",
    "                    assert len(label_each_char) == len(full_text)\n",
    "                    #print( label_each_char )\n",
    "\n",
    "            labels.append(label_each_char)\n",
    "            claim_offsets.append( (claim_start,claim_end) )\n",
    "            \n",
    "    return labels, claim_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5343b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for train dataframe\n",
    "labels_train, claim_offsets = get_char_labels(train_df)\n",
    "df_train['labels_char'] = labels_train\n",
    "df_train['claim_offsets'] = claim_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e5d309a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>stage2_labels</th>\n",
       "      <th>text</th>\n",
       "      <th>labels_char</th>\n",
       "      <th>claim_offsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sn9u41</td>\n",
       "      <td>t5_2s23e</td>\n",
       "      <td>I read an old thread on here that someone said...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Tysabri experiences\\nHi all\\n\\nI just had my 3...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>(508, 724)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p7j49y</td>\n",
       "      <td>t5_2syer</td>\n",
       "      <td>I have read that gout can't be cured, that it'...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Crazy amount of sardines caused gout (possibly...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>(1401, 1654)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smgy0q</td>\n",
       "      <td>t5_2s3g1</td>\n",
       "      <td>I always read stories of people who suffer fro...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Im sorry for intruding but I just want to say ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>(302, 419)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sxglhl</td>\n",
       "      <td>t5_2s3g1</td>\n",
       "      <td>Our results indicate that the addition of prob...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Is Bacillus coagulans supplementation plus low...</td>\n",
       "      <td>[0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>(1442, 1617)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rxyk1d</td>\n",
       "      <td>t5_2s1h9</td>\n",
       "      <td>Ive read that amnesia always followed a tonic ...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>[deleted by user]\\n[removed]</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>N.A.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id subreddit_id                                              claim  \\\n",
       "0  sn9u41     t5_2s23e  I read an old thread on here that someone said...   \n",
       "1  p7j49y     t5_2syer  I have read that gout can't be cured, that it'...   \n",
       "2  smgy0q     t5_2s3g1  I always read stories of people who suffer fro...   \n",
       "3  sxglhl     t5_2s3g1  Our results indicate that the addition of prob...   \n",
       "4  rxyk1d     t5_2s1h9  Ive read that amnesia always followed a tonic ...   \n",
       "\n",
       "                                       stage2_labels  \\\n",
       "0  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "1  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "2  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "3  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "4  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Tysabri experiences\\nHi all\\n\\nI just had my 3...   \n",
       "1  Crazy amount of sardines caused gout (possibly...   \n",
       "2  Im sorry for intruding but I just want to say ...   \n",
       "3  Is Bacillus coagulans supplementation plus low...   \n",
       "4                       [deleted by user]\\n[removed]   \n",
       "\n",
       "                                         labels_char claim_offsets  \n",
       "0  [2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, ...    (508, 724)  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  (1401, 1654)  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    (302, 419)  \n",
       "3  [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  (1442, 1617)  \n",
       "4                                               N.A.          N.A.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf3630cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_labels(df):\n",
    "    \n",
    "    tokens_series = []\n",
    "    labels_series = []\n",
    "    \n",
    "    for counter, row in enumerate(df):\n",
    "        #print('--------------------------------------------------------')\n",
    "\n",
    "        reddit_id = row['subreddit_id']\n",
    "\n",
    "        post_id = row['post_id']\n",
    "        claim = row['claim']\n",
    "        full_text = row['text']\n",
    "        char_labels = row['labels_char']\n",
    "        \n",
    "        tokens = []\n",
    "        token_labels = []\n",
    "\n",
    "\n",
    "        if 'N.A.' not in char_labels:\n",
    "            assert len(full_text) == len(char_labels)\n",
    "            \n",
    "            tokenized_text = [(m.group(0), m.start(), m.end() - 1) for m in re.finditer(r'\\S+', full_text)]\n",
    "            \n",
    "            for counter, i in enumerate(tokenized_text):\n",
    "                start = i[1]\n",
    "                end = i[2] + 1 \n",
    "                char_to_token_lab = list(set(char_labels[ start : end ]))\n",
    "                if len(char_to_token_lab) == 1:\n",
    "                    tokens.append( i[0] )\n",
    "                    token_labels.append( char_to_token_lab[0] )\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    #tokenize further\n",
    "                    new_text = tokenized_text[counter][0]\n",
    "                    new_labels = char_labels[ start : end ]\n",
    "                    #print(new_text , ' : ', new_labels)\n",
    "                    \n",
    "                    v = np.array( new_labels )\n",
    "                    tok_ind = np.where(np.roll(v,1)!=v)[0]\n",
    "                    tok_ind = list(tok_ind)\n",
    "                    if 0 not in tok_ind:\n",
    "                        tok_ind = [0] + tok_ind\n",
    "\n",
    "                    \n",
    "                    new_text_tokens = [new_text[i:j] for i,j in zip(tok_ind, tok_ind[1:]+[None])]\n",
    "                    new_text_labels = [new_labels[i:j] for i,j in zip(tok_ind, tok_ind[1:]+[None])]\n",
    "                    \n",
    "                    for t, l in zip(new_text_tokens, new_text_labels):\n",
    "                        tokens.append( t )\n",
    "                        token_labels.append( list(set(l))[0] )\n",
    "                        \n",
    "        else:\n",
    "            tokens.append( ['N.A.'] )\n",
    "            token_labels.append( ['N.A.'] )              \n",
    "                        \n",
    "        tokens_series.append(tokens)\n",
    "        labels_series.append(token_labels)\n",
    "                        \n",
    "    return tokens_series, labels_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "105efcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for train dataframe\n",
    "train_df = df_train.to_dict('records')\n",
    "text_tokens, token_labels = get_token_labels(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "532bed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tokens'] = text_tokens\n",
    "df_train['labels'] = token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3523cede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>stage2_labels</th>\n",
       "      <th>text</th>\n",
       "      <th>labels_char</th>\n",
       "      <th>claim_offsets</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sn9u41</td>\n",
       "      <td>t5_2s23e</td>\n",
       "      <td>I read an old thread on here that someone said...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Tysabri experiences\\nHi all\\n\\nI just had my 3...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>(508, 724)</td>\n",
       "      <td>[Tysabri, experiences, Hi, all, I, just, had, ...</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p7j49y</td>\n",
       "      <td>t5_2syer</td>\n",
       "      <td>I have read that gout can't be cured, that it'...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Crazy amount of sardines caused gout (possibly...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>(1401, 1654)</td>\n",
       "      <td>[Crazy, amount, of, sardines, caused, gout, (p...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smgy0q</td>\n",
       "      <td>t5_2s3g1</td>\n",
       "      <td>I always read stories of people who suffer fro...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Im sorry for intruding but I just want to say ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>(302, 419)</td>\n",
       "      <td>[Im, sorry, for, intruding, but, I, just, want...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sxglhl</td>\n",
       "      <td>t5_2s3g1</td>\n",
       "      <td>Our results indicate that the addition of prob...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>Is Bacillus coagulans supplementation plus low...</td>\n",
       "      <td>[0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>(1442, 1617)</td>\n",
       "      <td>[Is, Bacillus, coagulans, supplementation, plu...</td>\n",
       "      <td>[0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rxyk1d</td>\n",
       "      <td>t5_2s1h9</td>\n",
       "      <td>Ive read that amnesia always followed a tonic ...</td>\n",
       "      <td>[{\"crowd-entity-annotation\":{\"entities\":[{\"end...</td>\n",
       "      <td>[deleted by user]\\n[removed]</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>N.A.</td>\n",
       "      <td>[[N.A.]]</td>\n",
       "      <td>[[N.A.]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id subreddit_id                                              claim  \\\n",
       "0  sn9u41     t5_2s23e  I read an old thread on here that someone said...   \n",
       "1  p7j49y     t5_2syer  I have read that gout can't be cured, that it'...   \n",
       "2  smgy0q     t5_2s3g1  I always read stories of people who suffer fro...   \n",
       "3  sxglhl     t5_2s3g1  Our results indicate that the addition of prob...   \n",
       "4  rxyk1d     t5_2s1h9  Ive read that amnesia always followed a tonic ...   \n",
       "\n",
       "                                       stage2_labels  \\\n",
       "0  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "1  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "2  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "3  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "4  [{\"crowd-entity-annotation\":{\"entities\":[{\"end...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Tysabri experiences\\nHi all\\n\\nI just had my 3...   \n",
       "1  Crazy amount of sardines caused gout (possibly...   \n",
       "2  Im sorry for intruding but I just want to say ...   \n",
       "3  Is Bacillus coagulans supplementation plus low...   \n",
       "4                       [deleted by user]\\n[removed]   \n",
       "\n",
       "                                         labels_char claim_offsets  \\\n",
       "0  [2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, ...    (508, 724)   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  (1401, 1654)   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    (302, 419)   \n",
       "3  [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  (1442, 1617)   \n",
       "4                                               N.A.          N.A.   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Tysabri, experiences, Hi, all, I, just, had, ...   \n",
       "1  [Crazy, amount, of, sardines, caused, gout, (p...   \n",
       "2  [Im, sorry, for, intruding, but, I, just, want...   \n",
       "3  [Is, Bacillus, coagulans, supplementation, plu...   \n",
       "4                                           [[N.A.]]   \n",
       "\n",
       "                                              labels  \n",
       "0  [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "4                                           [[N.A.]]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2fc93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the dataframe to a csv file\n",
    "\n",
    "write_parsed = '/mnt/nas2/data/systematicReview/semeval2023/data/parsed/st2_train_parsed.tsv'\n",
    "#df_train.to_csv(write_parsed, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa628eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'population': 1, 'intervention': 2, 'outcome': 3}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picos_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661834a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
